# configs/config.yaml
model:
  model_name_or_path: "bert-base-uncased"
  num_labels: 2

data:
  task_name: "mrpc"
  max_length: 128

training:
  num_train_epochs: 20
  per_device_train_batch_size: 32
  per_device_eval_batch_size: 64
  learning_rate: 1e-5
  optimizer_type: "adamw"
  seed: 42
  trainable_layers: [0, 1, 2]  # Specify indices of layers to fine-tune; adjust as needed
  train_mode: "attention_matrices"           # Options: "all", "attention_matrices", "ffn_matrices"
  d: 10                     # Dimension of the low-dimensional subspace

output:
  output_dir: "./outputs/experiment1"
  save_strategy: "epoch"    # Or "no" to indicate not saving the model